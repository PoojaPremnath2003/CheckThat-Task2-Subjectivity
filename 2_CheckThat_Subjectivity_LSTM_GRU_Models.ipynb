{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNh4ktEA1w2R",
        "outputId": "bca6652a-7213-4eb3-a45f-9c3966652f21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CheckThat-Task2-Subjectivity'...\n",
            "remote: Enumerating objects: 19, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 19 (delta 7), reused 18 (delta 6), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (19/19), 173.30 KiB | 5.78 MiB/s, done.\n",
            "Resolving deltas: 100% (7/7), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/pooja-premnath/CheckThat-Task2-Subjectivity"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic LSTM and BiLSTM\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qHpo3z-p2fuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Load data\n",
        "df_final = pd.read_csv(\"/content/CheckThat-Task2-Subjectivity/train_en.tsv\", sep='\\t')\n",
        "df_test_final = pd.read_csv(\"/content/CheckThat-Task2-Subjectivity/dev_test_en.tsv\", sep='\\t')\n",
        "df_submission_final = pd.read_csv(\"/content/CheckThat-Task2-Subjectivity/test_en.tsv\", sep='\\t')\n",
        "\n",
        "# Tokenize text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df_final['sentence'])\n",
        "X_train = tokenizer.texts_to_sequences(df_final['sentence'])\n",
        "X_test = tokenizer.texts_to_sequences(df_test_final['sentence'])\n",
        "X_submission = tokenizer.texts_to_sequences(df_submission_final['sentence'])\n",
        "\n",
        "max_length = 100  # Choose an appropriate max length\n",
        "X_train = pad_sequences(X_train, maxlen=max_length, padding='post')\n",
        "X_test = pad_sequences(X_test, maxlen=max_length, padding='post')\n",
        "X_submission = pad_sequences(X_submission, maxlen=max_length, padding='post')\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "df_final['Encoded_Label'] = label_encoder.fit_transform(df_final['label'])\n",
        "y_train = df_final['Encoded_Label']\n",
        "y_test = label_encoder.transform(df_test_final['label'])\n",
        "\n",
        "# Define RNN model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=100, input_length=max_length),\n",
        "    LSTM(units=128),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate on test data\n",
        "_, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy on test data: {accuracy}\")\n",
        "\n",
        "# Predict probabilities on submission data\n",
        "y_submission_prob = model.predict(X_submission)\n",
        "\n",
        "# Convert probabilities to class labels based on threshold (0.5)\n",
        "y_submission_pred = (y_submission_prob > 0.5).astype(int)\n",
        "y_submission_pred = label_encoder.inverse_transform(y_submission_pred.flatten())\n",
        "\n",
        "# Replace encoded labels with original labels\n",
        "df_submission_final['Predicted_Label'] = y_submission_pred\n",
        "\n",
        "# Save submission DataFrame to TSV with sentence_id and original labels\n",
        "df_submission_final[['sentence_id', 'Predicted_Label']].to_csv(\"submission_predictions.tsv\", sep='\\t', index=False)\n",
        "\n",
        "# Calculate macro average F1 score on test set\n",
        "y_test_prob = model.predict(X_test)\n",
        "y_test_pred = (y_test_prob > 0.5).astype(int)\n",
        "y_test_pred = y_test_pred.flatten()\n",
        "macro_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
        "print(f\"Macro Average F1 Score on test set: {macro_f1}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9YH2m7E2FtX",
        "outputId": "9a315167-0ecb-4c0f-9689-90aafe84047b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "24/24 [==============================] - 9s 182ms/step - loss: 0.6581 - accuracy: 0.6439 - val_loss: 0.6669 - val_accuracy: 0.6145\n",
            "Epoch 2/20\n",
            "24/24 [==============================] - 5s 215ms/step - loss: 0.6529 - accuracy: 0.6439 - val_loss: 0.6725 - val_accuracy: 0.6145\n",
            "Epoch 3/20\n",
            "24/24 [==============================] - 4s 153ms/step - loss: 0.6552 - accuracy: 0.6439 - val_loss: 0.6667 - val_accuracy: 0.6145\n",
            "Epoch 4/20\n",
            "24/24 [==============================] - 4s 155ms/step - loss: 0.6539 - accuracy: 0.6439 - val_loss: 0.6684 - val_accuracy: 0.6145\n",
            "Epoch 5/20\n",
            "24/24 [==============================] - 5s 212ms/step - loss: 0.6521 - accuracy: 0.6439 - val_loss: 0.6690 - val_accuracy: 0.6145\n",
            "Epoch 6/20\n",
            "24/24 [==============================] - 4s 183ms/step - loss: 0.6521 - accuracy: 0.6439 - val_loss: 0.6671 - val_accuracy: 0.6145\n",
            "8/8 [==============================] - 11s 51ms/step - loss: 0.7283 - accuracy: 0.4905\n",
            "Accuracy on test data: 0.49054276943206787\n",
            "16/16 [==============================] - 1s 49ms/step\n",
            "8/8 [==============================] - 0s 47ms/step\n",
            "Macro Average F1 Score on test set: 0.3231197771587744\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Load data\n",
        "df_final = pd.read_csv(\"/content/CheckThat-Task2-Subjectivity/train_en.tsv\", sep='\\t')\n",
        "df_test_final = pd.read_csv(\"/content/CheckThat-Task2-Subjectivity/dev_test_en.tsv\", sep='\\t')\n",
        "df_submission_final = pd.read_csv(\"/content/CheckThat-Task2-Subjectivity/test_en.tsv\", sep='\\t')\n",
        "\n",
        "# Tokenize text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df_final['sentence'])\n",
        "\n",
        "# Encode text data to sequences\n",
        "X_train = tokenizer.texts_to_sequences(df_final['sentence'])\n",
        "X_test = tokenizer.texts_to_sequences(df_test_final['sentence'])\n",
        "X_submission = tokenizer.texts_to_sequences(df_submission_final['sentence'])\n",
        "\n",
        "# Pad sequences to ensure uniform length\n",
        "max_sequence_length = max([len(seq) for seq in X_train + X_test + X_submission])\n",
        "X_train = pad_sequences(X_train, maxlen=max_sequence_length, padding='post')\n",
        "X_test = pad_sequences(X_test, maxlen=max_sequence_length, padding='post')\n",
        "X_submission = pad_sequences(X_submission, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(df_final['label'])\n",
        "y_test = label_encoder.transform(df_test_final['label'])\n",
        "\n",
        "# Build highly complex LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length))\n",
        "model.add(Bidirectional(LSTM(units=512, return_sequences=True)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Bidirectional(LSTM(units=256, return_sequences=True)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Bidirectional(LSTM(units=128)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=30, batch_size=128, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Predict probabilities for test set\n",
        "y_pred_probs = model.predict(X_test)\n",
        "\n",
        "# Convert probabilities to class labels\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = (y_pred == y_test).mean()\n",
        "print(f'Accuracy on Test Set: {accuracy}')\n",
        "\n",
        "# Calculate macro average F1 score\n",
        "f1 = f1_score(y_test, y_pred, average='macro')\n",
        "print(f'Macro Average F1 Score on Test Set: {f1}')\n",
        "\n",
        "# Predict probabilities for submission data\n",
        "y_submission_pred_probs = model.predict(X_submission)\n",
        "\n",
        "# Convert probabilities to class labels\n",
        "y_submission_pred = np.argmax(y_submission_pred_probs, axis=1)\n",
        "predicted_labels = label_encoder.inverse_transform(y_submission_pred)\n",
        "\n",
        "# Check unique predicted labels\n",
        "unique_labels = np.unique(predicted_labels)\n",
        "print(f'Unique Predicted Labels: {unique_labels}')\n",
        "\n",
        "# Create dataframe with sentence_id and predicted labels\n",
        "df_submission_predicted = pd.DataFrame({'sentence_id': df_submission_final['sentence_id'], 'predicted_label': predicted_labels})\n",
        "\n",
        "# Save dataframe to TSV file\n",
        "df_submission_predicted.to_csv('submission_predictions_complex_lstm_updated.tsv', sep='\\t', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YAgg3596JcF",
        "outputId": "c53e2cab-d4b8-4f2b-9240-f42e7b70c24a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "7/7 [==============================] - 126s 17s/step - loss: 0.6687 - accuracy: 0.6217 - val_loss: 0.8599 - val_accuracy: 0.4774\n",
            "Epoch 2/30\n",
            "7/7 [==============================] - 97s 14s/step - loss: 0.6712 - accuracy: 0.6386 - val_loss: 0.7409 - val_accuracy: 0.4774\n",
            "Epoch 3/30\n",
            "7/7 [==============================] - 99s 14s/step - loss: 0.6413 - accuracy: 0.6398 - val_loss: 0.6902 - val_accuracy: 0.4774\n",
            "Epoch 4/30\n",
            "7/7 [==============================] - 100s 15s/step - loss: 0.5126 - accuracy: 0.7542 - val_loss: 1.2068 - val_accuracy: 0.5967\n",
            "Epoch 5/30\n",
            "7/7 [==============================] - 104s 15s/step - loss: 0.2860 - accuracy: 0.9072 - val_loss: 1.2871 - val_accuracy: 0.5885\n",
            "Epoch 6/30\n",
            "7/7 [==============================] - 98s 14s/step - loss: 0.1595 - accuracy: 0.9422 - val_loss: 1.2502 - val_accuracy: 0.5844\n",
            "8/8 [==============================] - 14s 2s/step\n",
            "Accuracy on Test Set: 0.4773662551440329\n",
            "Macro Average F1 Score on Test Set: 0.3231197771587744\n",
            "16/16 [==============================] - 20s 1s/step\n",
            "Unique Predicted Labels: ['OBJ' 'SUBJ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# With Attention"
      ],
      "metadata": {
        "id": "o06qxV-_BPgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBC-r5QOFUNQ",
        "outputId": "c3e71f93-f0db-4652-ba4d-ca24b2bb5c99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/611.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/611.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (24.0)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow_addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow_addons\n",
            "Successfully installed tensorflow_addons-0.23.0 typeguard-2.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Define Attention layer\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\")\n",
        "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1), initializer=\"zeros\")\n",
        "        super(AttentionLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        et = K.squeeze(K.tanh(K.dot(x, self.W) + self.b), axis=-1)\n",
        "        at = K.softmax(et)\n",
        "        at = K.expand_dims(at, axis=-1)\n",
        "        output = x * at\n",
        "        return K.sum(output, axis=1)\n",
        "\n",
        "# Load data\n",
        "df_final = pd.read_csv(\"/content/CheckThat-Task2-Subjectivity/train_en.tsv\", sep='\\t')\n",
        "df_test_final = pd.read_csv(\"/content/CheckThat-Task2-Subjectivity/dev_test_en.tsv\", sep='\\t')\n",
        "df_submission_final = pd.read_csv(\"/content/CheckThat-Task2-Subjectivity/test_en.tsv\", sep='\\t')\n",
        "\n",
        "# Tokenize text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df_final['sentence'])\n",
        "\n",
        "# Encode text data to sequences\n",
        "X_train = tokenizer.texts_to_sequences(df_final['sentence'])\n",
        "X_test = tokenizer.texts_to_sequences(df_test_final['sentence'])\n",
        "X_submission = tokenizer.texts_to_sequences(df_submission_final['sentence'])\n",
        "\n",
        "# Pad sequences to ensure uniform length\n",
        "max_sequence_length = max([len(seq) for seq in X_train + X_test + X_submission])\n",
        "X_train = pad_sequences(X_train, maxlen=max_sequence_length, padding='post')\n",
        "X_test = pad_sequences(X_test, maxlen=max_sequence_length, padding='post')\n",
        "X_submission = pad_sequences(X_submission, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(df_final['label'])\n",
        "y_test = label_encoder.transform(df_test_final['label'])\n",
        "\n",
        "# Build LSTM model with Attention mechanism\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length))\n",
        "model.add(Bidirectional(LSTM(units=512, return_sequences=True)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(AttentionLayer())\n",
        "model.add(Dense(units=64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=30, batch_size=128, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Predict probabilities for test set\n",
        "y_pred_probs = model.predict(X_test)\n",
        "\n",
        "# Convert probabilities to class labels\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = (y_pred == y_test).mean()\n",
        "print(f'Accuracy on Test Set: {accuracy}')\n",
        "\n",
        "# Calculate macro average F1 score\n",
        "f1 = f1_score(y_test, y_pred, average='macro')\n",
        "print(f'Macro Average F1 Score on Test Set: {f1}')\n",
        "\n",
        "# Predict probabilities for submission data\n",
        "y_submission_pred_probs = model.predict(X_submission)\n",
        "\n",
        "# Convert probabilities to class labels\n",
        "y_submission_pred = np.argmax(y_submission_pred_probs, axis=1)\n",
        "predicted_labels = label_encoder.inverse_transform(y_submission_pred)\n",
        "\n",
        "# Check unique predicted labels\n",
        "unique_labels = np.unique(predicted_labels)\n",
        "print(f'Unique Predicted Labels: {unique_labels}')\n",
        "\n",
        "# Create dataframe with sentence_id and predicted labels\n",
        "df_submission_predicted = pd.DataFrame({'sentence_id': df_submission_final['sentence_id'], 'predicted_label': predicted_labels})\n",
        "\n",
        "# Save dataframe to TSV file\n",
        "df_submission_predicted.to_csv('submission_predictions_lstm_attention_custom.tsv', sep='\\t', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itXv2WnVBc3Z",
        "outputId": "a553cd21-9b5c-4603-aea6-3516cc0c73e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "7/7 [==============================] - 50s 7s/step - loss: 0.6648 - accuracy: 0.6422 - val_loss: 0.7205 - val_accuracy: 0.4774\n",
            "Epoch 2/30\n",
            "7/7 [==============================] - 44s 6s/step - loss: 0.6625 - accuracy: 0.6410 - val_loss: 0.7325 - val_accuracy: 0.4774\n",
            "Epoch 3/30\n",
            "7/7 [==============================] - 43s 6s/step - loss: 0.6534 - accuracy: 0.6410 - val_loss: 0.7560 - val_accuracy: 0.4774\n",
            "Epoch 4/30\n",
            "7/7 [==============================] - 44s 6s/step - loss: 0.6391 - accuracy: 0.6410 - val_loss: 0.7673 - val_accuracy: 0.4774\n",
            "8/8 [==============================] - 5s 468ms/step\n",
            "Accuracy on Test Set: 0.4773662551440329\n",
            "Macro Average F1 Score on Test Set: 0.3231197771587744\n",
            "16/16 [==============================] - 9s 542ms/step\n",
            "Unique Predicted Labels: ['OBJ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the number of samples in each class\n",
        "class_counts = df_final['label'].value_counts()\n",
        "print(class_counts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qku6B5bqFSB5",
        "outputId": "d67f7d3d-e54c-4af1-c861-833e02d049ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label\n",
            "OBJ     532\n",
            "SUBJ    298\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Define Attention layer\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\")\n",
        "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1), initializer=\"zeros\")\n",
        "        super(AttentionLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        et = K.squeeze(K.tanh(K.dot(x, self.W) + self.b), axis=-1)\n",
        "        at = K.softmax(et)\n",
        "        at = K.expand_dims(at, axis=-1)\n",
        "        output = x * at\n",
        "        return K.sum(output, axis=1)\n",
        "\n",
        "# Load data\n",
        "df_final = pd.read_csv(\"/content/CheckThat-Task2-Subjectivity/train_en.tsv\", sep='\\t')\n",
        "df_test_final = pd.read_csv(\"/content/CheckThat-Task2-Subjectivity/dev_test_en.tsv\", sep='\\t')\n",
        "df_submission_final = pd.read_csv(\"/content/CheckThat-Task2-Subjectivity/test_en.tsv\", sep='\\t')\n",
        "\n",
        "# Tokenize text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df_final['sentence'])\n",
        "\n",
        "# Encode text data to sequences\n",
        "X_train = tokenizer.texts_to_sequences(df_final['sentence'])\n",
        "X_test = tokenizer.texts_to_sequences(df_test_final['sentence'])\n",
        "X_submission = tokenizer.texts_to_sequences(df_submission_final['sentence'])\n",
        "\n",
        "# Pad sequences to ensure uniform length\n",
        "max_sequence_length = max([len(seq) for seq in X_train + X_test + X_submission])\n",
        "X_train = pad_sequences(X_train, maxlen=max_sequence_length, padding='post')\n",
        "X_test = pad_sequences(X_test, maxlen=max_sequence_length, padding='post')\n",
        "X_submission = pad_sequences(X_submission, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(df_final['label'])\n",
        "y_test = label_encoder.transform(df_test_final['label'])\n",
        "\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(df_final['label']), y=df_final['label'])\n",
        "\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Build more complex LSTM model with Attention mechanism\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length))\n",
        "model.add(Bidirectional(LSTM(units=512, return_sequences=True)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Bidirectional(LSTM(units=256, return_sequences=True)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(AttentionLayer())\n",
        "model.add(Dense(units=128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model with class weights\n",
        "model.fit(X_train, y_train, epochs=30, batch_size=128, validation_data=(X_test, y_test), callbacks=[early_stopping], class_weight=class_weights_dict)\n",
        "\n",
        "# Predict probabilities for test set\n",
        "y_pred_probs = model.predict(X_test)\n",
        "\n",
        "# Convert probabilities to class labels\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = (y_pred == y_test).mean()\n",
        "print(f'Accuracy on Test Set: {accuracy}')\n",
        "\n",
        "# Calculate macro average F1 score\n",
        "f1 = f1_score(y_test, y_pred, average='macro')\n",
        "print(f'Macro Average F1 Score on Test Set: {f1}')\n",
        "\n",
        "# Predict probabilities for submission data\n",
        "y_submission_pred_probs = model.predict(X_submission)\n",
        "\n",
        "# Convert probabilities to class labels\n",
        "y_submission_pred = np.argmax(y_submission_pred_probs, axis=1)\n",
        "predicted_labels = label_encoder.inverse_transform(y_submission_pred)\n",
        "\n",
        "# Check unique predicted labels\n",
        "unique_labels = np.unique(predicted_labels)\n",
        "print(f'Unique Predicted Labels: {unique_labels}')\n",
        "\n",
        "# Create dataframe with sentence_id and predicted labels\n",
        "df_submission_predicted = pd.DataFrame({'sentence_id': df_submission_final['sentence_id'], 'predicted_label': predicted_labels})\n",
        "\n",
        "# Save dataframe to TSV file\n",
        "df_submission_predicted.to_csv('submission_predictions_lstm_attention_updated.tsv', sep='\\t', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AICT1a9Gv6J",
        "outputId": "b6a1da02-1c68-4d0e-9b68-f4ec501a61cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.7029 - accuracy: 0.5530 Epoch 2/30\n",
            "7/7 [==============================] - 88s 13s/step - loss: 0.6979 - accuracy: 0.4614 - val_loss: 0.6926 - val_accuracy: 0.5226\n",
            "Epoch 3/30\n",
            "7/7 [==============================] - 88s 13s/step - loss: 0.6913 - accuracy: 0.5145 - val_loss: 0.6962 - val_accuracy: 0.4774\n",
            "Epoch 4/30\n",
            "7/7 [==============================] - 88s 13s/step - loss: 0.6959 - accuracy: 0.4880 - val_loss: 0.6920 - val_accuracy: 0.5021\n",
            "Epoch 5/30\n",
            "7/7 [==============================] - 87s 13s/step - loss: 0.6776 - accuracy: 0.5325 - val_loss: 0.7065 - val_accuracy: 0.5144\n",
            "Epoch 6/30\n",
            "7/7 [==============================] - 85s 12s/step - loss: 0.7749 - accuracy: 0.7325 - val_loss: 0.6983 - val_accuracy: 0.5267\n",
            "Epoch 7/30\n",
            "7/7 [==============================] - 84s 12s/step - loss: 0.5002 - accuracy: 0.7747 - val_loss: 0.8390 - val_accuracy: 0.6008\n",
            "8/8 [==============================] - 10s 1s/step\n",
            "Accuracy on Test Set: 0.5020576131687243\n",
            "Macro Average F1 Score on Test Set: 0.34146341463414637\n",
            "16/16 [==============================] - 16s 1s/step\n",
            "Unique Predicted Labels: ['OBJ' 'SUBJ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple GRU\n"
      ],
      "metadata": {
        "id": "TuwJKBSBJavd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Load data\n",
        "df_final = pd.read_csv(\"/content/CheckThat-Task2-Subjectivity/train_en.tsv\", sep='\\t')\n",
        "df_test_final = pd.read_csv(\"/content/CheckThat-Task2-Subjectivity/dev_test_en.tsv\", sep='\\t')\n",
        "df_submission_final = pd.read_csv(\"/content/CheckThat-Task2-Subjectivity/test_en.tsv\", sep='\\t')\n",
        "\n",
        "# Tokenize text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df_final['sentence'])\n",
        "\n",
        "# Encode text data to sequences\n",
        "X_train = tokenizer.texts_to_sequences(df_final['sentence'])\n",
        "X_test = tokenizer.texts_to_sequences(df_test_final['sentence'])\n",
        "X_submission = tokenizer.texts_to_sequences(df_submission_final['sentence'])\n",
        "\n",
        "# Pad sequences to ensure uniform length\n",
        "max_sequence_length = max([len(seq) for seq in X_train + X_test + X_submission])\n",
        "X_train = pad_sequences(X_train, maxlen=max_sequence_length, padding='post')\n",
        "X_test = pad_sequences(X_test, maxlen=max_sequence_length, padding='post')\n",
        "X_submission = pad_sequences(X_submission, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(df_final['label'])\n",
        "y_test = label_encoder.transform(df_test_final['label'])\n",
        "\n",
        "# Build a more complex GRU model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length))\n",
        "model.add(Bidirectional(GRU(units=512, return_sequences=True)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Bidirectional(GRU(units=256, return_sequences=True)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(GRU(units=128))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=30, batch_size=128, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Predict probabilities for test set\n",
        "y_pred_probs = model.predict(X_test)\n",
        "\n",
        "# Convert probabilities to class labels\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = (y_pred == y_test).mean()\n",
        "print(f'Accuracy on Test Set: {accuracy}')\n",
        "\n",
        "# Calculate macro average F1 score\n",
        "f1 = f1_score(y_test, y_pred, average='macro')\n",
        "print(f'Macro Average F1 Score on Test Set: {f1}')\n",
        "\n",
        "# Predict probabilities for submission data\n",
        "y_submission_pred_probs = model.predict(X_submission)\n",
        "\n",
        "# Convert probabilities to class labels\n",
        "y_submission_pred = np.argmax(y_submission_pred_probs, axis=1)\n",
        "predicted_labels = label_encoder.inverse_transform(y_submission_pred)\n",
        "\n",
        "# Check unique predicted labels\n",
        "unique_labels = np.unique(predicted_labels)\n",
        "print(f'Unique Predicted Labels: {unique_labels}')\n",
        "\n",
        "# Create dataframe with sentence_id and predicted labels\n",
        "df_submission_predicted = pd.DataFrame({'sentence_id': df_submission_final['sentence_id'], 'predicted_label': predicted_labels})\n",
        "\n",
        "# Save dataframe to TSV file\n",
        "df_submission_predicted.to_csv('submission_predictions_gru_updated.tsv', sep='\\t', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THYH4yzPIjET",
        "outputId": "7b5fee64-164a-4859-cf7d-dbe489c2bd5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "7/7 [==============================] - 76s 9s/step - loss: 0.6729 - accuracy: 0.6000 - val_loss: 0.6960 - val_accuracy: 0.4774\n",
            "Epoch 2/30\n",
            "7/7 [==============================] - 61s 9s/step - loss: 0.6760 - accuracy: 0.6012 - val_loss: 0.7603 - val_accuracy: 0.4774\n",
            "Epoch 3/30\n",
            "7/7 [==============================] - 62s 9s/step - loss: 0.6615 - accuracy: 0.6337 - val_loss: 0.7236 - val_accuracy: 0.4774\n",
            "Epoch 4/30\n",
            "7/7 [==============================] - 65s 9s/step - loss: 0.6659 - accuracy: 0.6313 - val_loss: 0.7496 - val_accuracy: 0.4774\n",
            "8/8 [==============================] - 8s 810ms/step\n",
            "Accuracy on Test Set: 0.4773662551440329\n",
            "Macro Average F1 Score on Test Set: 0.3231197771587744\n",
            "16/16 [==============================] - 14s 888ms/step\n",
            "Unique Predicted Labels: ['OBJ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ASSaQo2yOkn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RPGp6yFmS8kQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "07aHforYS8o5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X-Bw8ezqS8qh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}